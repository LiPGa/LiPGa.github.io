<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Design | L</title>
    <link>/categories/design/</link>
      <atom:link href="/categories/design/index.xml" rel="self" type="application/rss+xml" />
    <description>Design</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 05 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Design</title>
      <link>/categories/design/</link>
    </image>
    
    <item>
      <title>Perfume search engine</title>
      <link>/project/perfume/</link>
      <pubDate>Sun, 05 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/project/perfume/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;The process of searching perfumes and purchasing perfume are &lt;strong&gt;seperated&lt;/strong&gt; from each other. The perfume shopping website does not provide much detailed product information, such as the front, middle, and back notes, perfumers, etc. Similarly, perfume review site do not provide functionalities like navigation to the shopping page.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As a user who want to buy perfumes but are not familiar with them, I have to go to perfume review websites like Nosetime or Xiaohongshu to look for a perfume that I like. However, if I have decided what to buy, I have to search again on shopping website like Taobao.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is no specific perfume shopping website on the market, which brings a lot of trouble to users&amp;rsquo; demands.
Therefore, this perfume searching and shopping site is designed and developed to meet the personalized purchase needs of users by integrated and all-round information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User could search perfumes by text and image.&lt;/li&gt;
&lt;li&gt;User could be navigated to the shopping site in the result page.&lt;/li&gt;
&lt;li&gt;User could combine different features to dig out favourite product.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The text searching function is implemented through Apache Lucene. The image recognition is implemented using ORB operator and brute force matcher provided by OpenCV.&lt;/p&gt;
&lt;h2 id=&#34;interface&#34;&gt;Interface&lt;/h2&gt;
&lt;p&gt;Menu Page:
&lt;img src=&#34;3.png&#34; alt=&#34;&#34;&gt;
Search by Text:
&lt;img src=&#34;2.png&#34; alt=&#34;&#34;&gt;
Search by Image:
&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;
Muti-field Search:
&lt;img src=&#34;4.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;6.png&#34; alt=&#34;&#34;&gt;
Search Result:
&lt;img src=&#34;5.png&#34; alt=&#34;&#34;&gt;
&lt;img src=&#34;7.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;more-features&#34;&gt;More Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Leverage regular crawlers of Taobao and Nosetime to collect information on a daily basis to keep the database information up-to-date.&lt;/li&gt;
&lt;li&gt;Fuzzy Search to improve the robustness of the seaching function.&lt;/li&gt;
&lt;li&gt;Offer suggestions based on search and web history before user finish typing.&lt;/li&gt;
&lt;li&gt;Price prediction, personalized product recommendation based on the data collected.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Voice Navigation APP for Blind User</title>
      <link>/post/android/</link>
      <pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/android/</guid>
      <description>&lt;h2 id=&#34;purpose&#34;&gt;Purpose&lt;/h2&gt;
&lt;p&gt;With the rapid development of mobile applications，Android—based mobile applications constantly innovation，various Android application store in the number of applications is increasing．But how to use innovative applications to help people with disabilities，is currently a major focus of the problem. Nowadays, apps designed to meet the special needs of blind people are restricted in numbers, and our project–“Seeing eye dog” is targeted at such need to offer a third eye for the blind people.&lt;/p&gt;
&lt;h2 id=&#34;logo&#34;&gt;Logo&lt;/h2&gt;
&lt;p&gt;Our app name is Seeing-eye dog, which means it is a good friend of the blind people.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;logo.png&#34; alt=&#34;&#34;&gt;
The four circles stand for the four members of our group. It also represents the all-round communication and travel of blind people with the help of our app. &amp;ldquo;The third eye for the blind&amp;rdquo; is our motto.&lt;/p&gt;
&lt;h2 id=&#34;platform-and-development-environment&#34;&gt;Platform and Development environment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Android 7.0&lt;/li&gt;
&lt;li&gt;Android Studio 2.3.2&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;function-implementation&#34;&gt;Function Implementation&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR
id1(applying sdk) --&amp;gt; id2(location)
id3(voice input) --&amp;gt; id4(navigation) 
id2 --&amp;gt; id5(vibration output) 
id4 --&amp;gt; id5
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;location-and-navigation&#34;&gt;Location and navigation&lt;/h3&gt;
&lt;p&gt;There is one advantage of mobile device over PC is that it can be taken with you easily. Therefore, LBS, short for Location Based Service is a technology almost only available on mobile phones. The core of LBS is to locate users. There are mainly two ways to achieve this goal, that is, through GPS and through WiFi. The ﬁrst method is based on the interaction of GPS hardware inside our phones and satellites. Users can be located precisely in this way but it is only useful outdoors. The second method depends on three base-stations to determine the velocity and then calculate the positon. This way is less accurate but is available both indoors and outdoors. Although android has provided corresponding API support for both two ways, there are some problems in practice. To improve accuracy as well as save time, we choose the third way, using SDK of third companies (AMAP). The procedure is applying API Key first, then preparing LBS SDK, and ﬁnally we can design our own application.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR
id1(apply API key) --&amp;gt; id2(Prepare LBS SDK) --&amp;gt; id3(design application) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then if all of them are accepted, we can begin our location. We call some inside method to get latitude and longitude, as well as the exact city and road to tell users where they are in a more acceptable way. The time interval is used to set interval of two location, through which we can change our position synchronous in the map and get information of the whole trail we have moved visualized.&lt;/p&gt;
&lt;p&gt;Once user presses the left button, a complete map will be showed; if user presses the right one, the application will turn back to the main interface. In the map before, user can zoom in and out by sliding their fingers or pressing &amp;ldquo;+&amp;rdquo; or &amp;ldquo;-&amp;quot;.&lt;/p&gt;
&lt;h3 id=&#34;voice-recognition&#34;&gt;Voice Recognition&lt;/h3&gt;
&lt;p&gt;This function is fulfilled to assist the blind as well as other users to locate their destinations with higher speed and more convenience. Through the calling of &lt;code&gt;Hearing Fei voice SDK&lt;/code&gt;，accurate and efficient voice recognition is accomplished.&lt;/p&gt;
&lt;p&gt;To provide vivid and real-time interactive experience,&lt;/p&gt;
&lt;p&gt;The result is satisfactory for the high accuracy and vivid interface.&lt;/p&gt;
&lt;h3 id=&#34;vibration-output&#34;&gt;Vibration Output&lt;/h3&gt;
&lt;p&gt;This function is specially designed for blind people. The blind people boasts sensitive tactile and auditory. However, because of the noise generated by busy traffic on the road, it is practically rather difficult for the blind people to know their location and identify their environment through apps simply by their hearing. For example, if simply focusing on the app wearing a headphone, it is difficult for them to identify the horns of vehicles, which may otherwise be useful for them to navigate for some safety concern.&lt;/p&gt;
&lt;p&gt;To solve this problem, vibration is adopted in our APP, for its &lt;strong&gt;in-time&lt;/strong&gt; and &lt;strong&gt;noise proof&lt;/strong&gt; advantages, which  perfectly fits in with the features and demands of blind users. To be specific, the method of &lt;strong&gt;difference-vibration&lt;/strong&gt; is implemented:
&lt;img src=&#34;vibrate.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Short vibration:
&lt;ul&gt;
&lt;li&gt;Waiting time:1s&lt;/li&gt;
&lt;li&gt;Lasting time:0.05s&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Long vibration:
&lt;ul&gt;
&lt;li&gt;Waiting time:1s&lt;/li&gt;
&lt;li&gt;Lasting time:1s&lt;/li&gt;
&lt;li&gt;The vibration can achieve its peak.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ui-design&#34;&gt;UI design&lt;/h3&gt;
&lt;p&gt;Since the users are divided into 2 groups — blind and normal people. Ways of input includes typing input and voice-input. Their texts are all clearly replaced by icon buttons, to make the interface more attractive, some additional decorations are rejected to make the interface more user-friendly. In the lower right hand corner is our app logo and in the lower left hand corner is user’s command and it also supports voice-input.&lt;/p&gt;
&lt;h4 id=&#34;main-interface--position-track&#34;&gt;Main interface &amp;amp; Position track&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;voice-recognition-1&#34;&gt;Voice recognition&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;voice.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Our completion of the final project, thanks to the joint effort, is satisfying. We accomplished following features:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Navigation function via Amap and location sdk.&lt;/li&gt;
&lt;li&gt;Voice recognition and vibration output to receive and deliver information.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is our first time to experience android development, and naturally, this project has a lot to improve:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Without voice output, it is still not convenient enough to use ”Seeing Eye dog”.&lt;/li&gt;
&lt;li&gt;The functions are limited in numbers and could be made more diverse, such as travel sharing, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary-and-future-perspectives&#34;&gt;Summary and Future perspectives&lt;/h2&gt;
&lt;p&gt;Through this project, we have fully experienced the fun and sense of achievements of accomplishing an visible outcome and fall in love with android development. Limited by the time, many additional functions are not be able to implemented. If added, &amp;ldquo;Seeing-Eye dog&amp;rdquo; app will undoubtedly have more practical use and enrich the life of the user groups to a larger degree. We hope to have the opportunity to turn these functions into realities in the future.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;More services closely related to the life of the blind can be added. For example, the restaurants and entertainment facilities nearby to enrich the otherwise dull life of the blind people.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Personally made service can be added to be more intellect, and suit users’ demands better.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Share function can be added to enable the blind to share their travelling path among their friends and families&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To sum up, technology will improve and enrich people’s life. We should tap the potential of the technology to make more products better satisfying the special need of those underprivileged who have long been easily ignored. And only by doing so, can the technology be made full use of, thus eventually changing the world into a better place for all humanity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment Monitoring</title>
      <link>/project/sentiment/</link>
      <pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/project/sentiment/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;On GitHub, issue reports are used by team members to ask for advice, and express and share opinions related to software maintenance and evolution.
Issue data can be mined to explore developers emotions, sentiments and politeness&amp;mdash;affects for short.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It is becoming apparent that emotion awareness, awareness of one&#39;s own emotions and those of others, is crucial for a software company and for many stakeholders involved in the software development lifecycle.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is important to improve Emotion or Sentiment awareness in Open Source Software developments. Although monitoring systmes for emotion phenomena detection has been studied in social media, there is no studies on tools to monitor sentiment phenomena in software development communities.&lt;/p&gt;
&lt;p&gt;This design could provide an alternative view to evaluate the health status of open source software projects (from a social and pshychological point of view).&lt;/p&gt;
&lt;h2 id=&#34;workflow&#34;&gt;Workflow&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;system.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All the data is automatically collected using &lt;a href=&#34;https://developer.github.com/v3/&#34;&gt;REST API v3&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Sentiment Analysis is performed using SentiCR and an SE-specific sentiment lexicon.&lt;/li&gt;
&lt;li&gt;Burst detection is implemented through Kleinburg model and Event extraction is fulfilled through LDA Topic extraction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-analysis-and-visualization&#34;&gt;Data Analysis and Visualization&lt;/h2&gt;
&lt;p&gt;Visualization of sentiment status of all the &lt;strong&gt;commit&lt;/strong&gt; comments in a given repository (individual work):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;vscode.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Visualization of all the &lt;strong&gt;issue&lt;/strong&gt; comments in a given repository (collaborative work):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sentiment-analysis.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;These two graphs can provide a general idea of the sentiment status of the project by simply displaying the generated time series and summary information. Based on them, I further design some interfaces for the sentiment monitoring system.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;As the Project Manager, I want to know whether there is a burst of a certain sentiment and what events lead to such sentiment phenomena.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Prototype of Burst Detection and Event Extraction:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detected events could link to corresponding GitHub pages, enabling user to know details of the event.&lt;/li&gt;
&lt;li&gt;User could select a specific time period and the system will detect sentiment burst and related events during this period. Typical comments will also be listed with sentiment score and keywords displayed to the user.&lt;/li&gt;
&lt;li&gt;For better accuracy, user could adjust number of events to be detected by the model during the burst period.&lt;/li&gt;
&lt;li&gt;User could watch the project he/she is interested in. Whenever there is new detection, user could receive notifications.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;sentiment.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As the Software Quality Assurance team member, I want to see the overall sentiment state of the project in the past period such as one week, one month, one year, etc., so that I can correlate it with other metrics, such as productivity, task quality, etc.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Prototype of information visualization and summary on a given time period:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Enabling sliding window for adaptive visualization.&lt;/li&gt;
&lt;li&gt;Provide selection on time period and look for database or automatically collect data during that period from GitHub.&lt;/li&gt;
&lt;li&gt;Provide issue and commit comment data summary.&lt;/li&gt;
&lt;li&gt;Automatically compare the result with the summary over the last period.&lt;/li&gt;
&lt;li&gt;Give advice based on some embeded knowledge. (e.g, Since research has found that positive sentiment can reduce issue resolution time, the system can help QA team member to evaluate productivity accordingly.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;visualize.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
